

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
### Name:YUVAN SUNDAR S
### REG.NO:212223040250
## Aim:
To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

## AI Tools Required: 
- ChatGPT
- Document editor for recording responses
- Evaluation rubric (quality, accuracy, depth)
-------
## Prompt Types and Definition 
- Naive Prompt (Broad/Unstructured): Vague, open-ended, lacks specific instructions, context, length, tone, or format requirements.
- Basic Prompt (Clearer/Refined): Clear, detailed, structured, includes key instructions: Role, Task, Constraints (Length/Tone/Format), and Context.

## Procedure
1. For each test scenario, design two prompts: naïve and basic.
2. Collect ChatGPT responses for both.
3. Compare results using evaluation criteria.
------------

## Test Scenarios & Example Prompts:
### Sccenario 1: Creative Story Generation

 **Naïve Prompt:** "Write a story."
 
 **Observation:** The response was a Generic, unstructured narrative with no defined plot, characters, or specific theme.
 
 **Basic Prompt:** "Write a short story (150–200 words) about a young engineer who builds a solar-powered car to help her village. The story should have a beginning, conflict, and resolution."
 
 **Observation:** The response provided a Clear context, detailed plot, and structured storyline, adhering to the requested length and narrative components.

----------------------

### Scenario 2: Factual Question 
**Naïve Prompt**: "Tell me about electricity."

**Observation**: The response was Overly broad and scattered, often defaulting to a historical overview instead of a technical explanation.

**Basic Prompt**: "Explain electricity in 3–4 lines, focusing on its definition, sources, and everyday uses. Keep it simple for a high school student."

**Observation**: The response was Concise, accurate, and audience-specific, hitting all required points in the specified format and length.

------------------------

### Scenario 3: Summarization 
**Naïve Prompt**: "Summarize AI."

**Observation:** The response was a Short, vague answer, usually providing only a basic, single-sentence definition.

**Basic Prompt:** "Summarize the concept of Artificial Intelligence in 5 sentences, covering definition, types, key applications, and its importance in modern technology."

**Observation:** The response was Organized, comprehensive, and well-structured, covering all the specified key points efficiently.

------------------------

### Scenario 4: Advice / Recommendation 
**Naïve Prompt:** "Give me advice."

**Observation:** The response offered Random and unfocused suggestions that lacked context or immediate applicability.

**Basic Prompt:** "Give me 3 practical tips on how a college student can manage time effectively while preparing for exams."

**Observation:** The response was Specific, actionable, and relevant, directly addressing the user (college student) and the specific problem (exam time management).

----------------------

### Comparative Table
| Scenario                  | Naïve Prompt Response        | Basic Prompt Response                 | Quality | Accuracy | Depth  |
| ------------------------- | ---------------------------- | ------------------------------------- | ------- | -------- | ------ |
| **Story Generation**      | Generic story, no structure  | Clear story with context & resolution | Medium  | Medium   | Medium |
| **Factual Question**      | Broad, unfocused explanation | Concise, accurate summary             | High    | High     | High   |
| **Summarization**         | Vague definition             | Structured, covers all key points     | Medium  | High     | High   |
| **Advice/Recommendation** | General, unfocused           | Actionable, 3 clear tips              | Medium  | High     | High   |

------------------------
### Analysis and Summary of Findings:
**Analysis**
- **Quality:** Improved consistently with structured prompts. The defined constraints (word count, tone, format) forced the AI to produce a more polished and usable output.

-**Accuracy:** Basic prompts significantly reduced irrelevant or vague outputs, ensuring the response directly addressed the core task.

- **Depth:** Detailed prompts elicited richer, context-aware responses, particularly in the Summarization and Factual Question scenarios, where multiple specific points were requested.

- **Key Insight:** While naïve prompts can sometimes generate creative variety, basic prompts consistently yield better accuracy and structure by eliminating ambiguity for the model.

### Summary of Findings (Optimal Prompting)
Prompt clarity strongly impacts output quality and reliability.

1. Naïve prompts lead to generic or incomplete answers that require further refinement.

2. Basic prompts guide the AI to provide focused, audience-specific, and actionable outputs.

3. For best results, always include context, constraints, and expected format in prompts.

---------------------------------

# OUTPUT:
The naïve prompts produced generic, less structured, and sometimes vague responses. The basic prompts delivered clear, accurate, and context-rich outputs with better depth and audience focus. Across all test scenarios (story generation, factual Q&A, summarization, advice), basic prompts consistently outperformed naïve prompts in terms of quality, accuracy, and depth.
# RESULT:
The prompt for the above said problem executed successfully
